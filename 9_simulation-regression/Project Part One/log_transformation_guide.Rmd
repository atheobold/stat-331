---
output:
  pdf_document: default
  html_document: default
---

## Log Transformation Interpretations

When you are in the case where you have log transformed your $x$ variable, 
your regression equation will look like the following:

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times log(x)$$


### Intercept

The intercept estimate ($\hat{\beta}_0$) represents the estimated mean value of
$y$ when the explanatory variable is 0. However, in a $log(x)$ scenario, you
cannot evaluate this when $x = 0$, since $log(0) = \infty$. So, we need to 
evaluate the equation when $log(x) = 0,$ which occurs when $x = 1.$

Thus, the intercept is interpreted as the expected mean value of $y$ when
$x = 1.$

### Slope

For the slope interpretation, we rely on the properties of logarithms. We are 
interested in increasing $x$ and seeing what the associated change in $y$ is. 
Typically, in a linear regression we increase $x$ by 1 unit and $\hat{\beta}_1$
is the expected change in the mean of $y.$ 

However, when we have $log(x)$ we need to increase $x$ slightly differently. 
This is because $log(x + 1) \neq log(x) + log(1).$ So, we need to figure out a 
way to increase $x$ that allows for us to separate $log(x)$ from the change in 
$y.$

From properties of logarithms, we know $log(2x) = log(2) \times log(x).$ Thus, 
if we double $x$ we get the following:

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times log(2x)$$
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times \Big(log(2) \times log(x)\Big)$$
$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times log(2) + \hat{\beta}_1 \times log(x)$$

\vspace{1cm}

Notice that our original equation was 
$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 \times log(x).$ So, by doubling $x$ 
we saw a $\hat{\beta}_1 \times log(2)$ change in the mean of $y.$ 

