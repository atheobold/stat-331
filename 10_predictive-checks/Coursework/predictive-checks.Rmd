---
title: "Predictive Checks"
author: ""
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center")

library(tidyverse)
library(gridExtra)
library(broom)

set.seed(0114)
```

This week you will learn:

- how to make predictions from a linear model
- how to include variability into predictions
- what is assumed about the data generating process in a linear regression
- how to assess if the assumed linear model accurately describes the observed
data

---

<center>
**This week everything is self-contained in this coursework page!** 
</center>

`r emo::ji("book")` Readings: 15-30 minutes  

`r emo::ji("computer")` Activities: 10-15 minutes
  
`r emo::ji("check")` Check-ins: 5 (combined in 1 Canvas quiz)

---

> It is difficult to include in a [statistical model] all of oneâ€™s
> knowledge about a problem, and so it is wise to investigate what aspects of
> reality are not captured by the model.  
> - Andrew Gelman, *Bayesian Data Analysis* 

# Model Checking

In explanatory modeling, it is incredibly important to check if you can make 
sound inferences from the model that was fit. Whether the model is a "simple" 
difference in means or a more "complex" multiple linear regression, the 
estimated coefficients obtained from the model should not be used for inference
if the conditions of the model are violated. Typically, model checking for a 
linear regression is taught through a "residual analysis," inspecting different
visualizations of the residuals from the model. This week, however, we will 
learn a different way to assess our regression model. 

## Predictive Checks, Bayesian Modeling, and Subjectivity

The idea of **_predictive checks_** for a statistical model are most often seen in 
the context of Bayesian modeling. In Bayesian modeling, rather than obtaining a 
single estimate for a parameter, you obtain a distribution of plausible values
for that parameter. The distribution suggests which values of that parameter are
more or less likely, given what was seen in the data and the prior knowledge
incorporated into the model. 

The prior knowledge about what values we expect for the parameter comes in the
form of a **_prior distribution_**. Different statisticians may choose a different
prior distributions, much like different statisticians might choose different 
variables to include in their regression. Although there are large similarities
between the choice of a prior distribution and the choice of a statistical
model, prior distributions receive a great deal of flack for how the interlace
"subjectivity" into the statistical modeling process. 

Due in part to these criticisms regarding the choice of a prior distribution, it
is common in Bayesian analyses to perform predictive checks to assess the fit of
the model to the data at hand. While we are not fitting Bayesian models, I
believe there is a great deal we can learn from the concept of using predictive
checks for **any** statistical model. Any good analysis should include a check
of the "adequacy of the fit of the model to the data and the plausibility of the
model for the purposes for which the model will be used" (Gelman et al., 2020,
p. 141). 


## Predictive Checks in a Linear Regression 

Under a linear regression model, we assume that the responses can be modeled as
a function of the explanatory variables and some error: 
$$
\text{y} = \beta_0 + \beta_1 \times x_1 + \cdots + \beta_n \times x_n + \epsilon
$$

Specifically, in a linear model we assume that the errors ($\epsilon$) follow a 
Normal distribution with mean 0 and standard deviation $\sigma$
($\epsilon \sim N(0, \sigma)$). 

This implies that we can simulate data that we would have expected 
to come from this model, by adding normally distributed errors to the values 
predicted from the linear regression. By randomly generating these errors, we
better understand what the "normality," "equal variance," and "independence"
conditions mean.

First, we are assuming that the errors follow a Normal distribution, because
that is the distribution we are using to generate the errors! Second, we are 
assuming that the variability of **every** observation is the same, which is 
why we can use the same value of $\sigma$ when generating errors. Finally, we
are assuming that we can randomly distribute the errors to the
different predictions, because we are assuming that the observations are
independent. 

The independence assumption is a bit more of a stretch, so let's break down 
the components. When we assume that observations are independent, we are saying 
that we shouldn't be able to know the $y$ value for one observation just from 
knowing the $y$ value of another observation. If I have repeated observations 
on the same person taking a memory test, then it's pretty likely that I can
guess their other scores from knowing one of their scores. Since we are using a
linear regression to model the relationship between the explanatory variable(s)
and the response, this assumption can be rephrased in terms of each
observation's residual. Specifically, knowing the residual for one observation
shouldn't give us perfect information about the residual for another
observation. Because we are assuming that the observations / residuals are 
independent, we can randomly draw an errors for each observation. 


---


# Performing a Predicitive Check for a Linear Regression

I will be walking through how to carry out the process of performing a
predictive check for a multiple linear regression model in the context of the 
`penguins` dataset from the **palmerpenguins** package. 

We will start by fitting a multiple linear regression model, modeling a 
penguin's `bill_length` as a function of their `bill_depth` and `species`. 

```{r}
library(palmerpenguins)

penguin_lm <- penguins %>% 
  lm(bill_length_mm ~ bill_depth_mm*species, data  = .)
```


## Obtaining Predictions

The next step is to obtain the predictions (fitted values) from the regression 
model. There is a handy built-in function for extracting the predicted values 
from a `lm` object -- the `predict()` function. 

```{r}
penguin_predict <- predict(penguin_lm)
```

`r emo::ji("check")` **Check-in: The `predict()` function**

What is the data structure of the predictions that are output by the `predict()`
function?

(a) list
(b) vector
(c) matrix
(d) dataframe


## Extracting the Estimate of $\sigma$

The residual standard error shown in the `summary()` output of a linear 
regression is the "best" estimate for the value of $\sigma$. We can 
extract this value using the built-in `sigma()` function and save it in an 
object for later use. 

```{r}
penguin_sigma <- sigma(penguin_lm)
```


## Adding Noise to Predictions

As stated before, under a linear regression model, we can simulate data that we
would have expected to come from this model, by adding normally distributed
errors to the values predicted from the linear regression.

Thus, it is useful for us to write a function that adds Normally distributed 
errors to a vector, `x`, given the value of `sd` input. I've written one such 
function here: 

```{r}
noise <- function(x, mean = 0, sd){
  n <- length(x)
  new_data <- x + rnorm(n, mean, sd)
  return(new_data)
}
```

Next, we can use this function to generate a new dataset with observations 
we would have expected to come from our linear regression. Note I'm storing 
these predictions in a `tibble()`, so that I can easily plot them later! 

```{r}
new_data <- tibble(predict_length = noise(penguin_predict, 
                                          sd = penguin_sigma)
                   )
```

`r emo::ji("check")` **Check-in: Normal Errors**

When simulating data that come from a linear regression, why should the default value for the `mean` be 0? (when passed into the `rnorm()` function)

---

# Plotting Predictions 

The best way to assess if / where there are differences between the simulated 
data and the observed data is through visualizations. We can do this in two
ways, (1) visualizing the distribution of the responses, and (2) visualizing 
the relationship between the responses and the explanatory variables. 

## Distribution of Responses

```{r}
pred <- new_data %>% 
  ggplot(aes(x = predict_length)) +
  geom_histogram(binwidth = 3.5) +
  labs(x = "Simulated Bill Lengths (mm)") +
  xlim(25, 65)

obs <- penguins %>% 
  ggplot(aes(x = bill_length_mm)) + 
  geom_histogram(binwidth = 3.5) + 
  labs(x = "Observed Bill Lengths (mm)") + 
  xlim(25, 65)


gridExtra::grid.arrange(pred, obs, nrow = 1)
```

`r emo::ji("check")` **Check-in: Simulated Data Distribution**

Are the simulated bill lengths similar or different from the observed bill
lengths? Where do you see differences? Where do you see similarities? Would 
you conclude that both distributions could have come from the same data 
generating process? 


## Adding in the Observed x's 

If we are interested in seeing if the observed relationships between 
`bill_length`, `bill_depth`, and `species` are radically different from the 
simulated data, we need to add these variables into our simulated dataset. 

Remember, by default `lm()` throws out any observations with missing values 
(`NA`) for any of the variables included in the regression. Thus, we will have 
fewer predictions than observations in our dataset. To make sure the predictions
match up with their corresponding row, we need to filter the missing values out 
of the dataset before we bind the columns together. 


```{r}
new_data <- penguins %>% 
  filter(!is.na(bill_depth_mm), 
         !is.na(bill_length_mm), 
         !is.na(species)) %>% 
  select(bill_length_mm, bill_depth_mm, species) %>% 
  bind_cols(new_data)

```

## Scatterplot of Relationships

A scatterplot of the relationships modeled by the linear regression can give a 
more detailed idea for where the simulated data differ from the observed data. 
In the scatterplots below, we see that the data simulated from the regression 
model are much more muted, meaning there are fewer "extreme" observations. For
example, in the Gentoo penguins, there are fewer observations of long bills. 


```{r, fig.width = 10, out}
pred <- new_data %>% 
  ggplot(aes(y = predict_length, x = bill_depth_mm, color = species)) + 
  geom_point() + 
   labs(title = "Simulated Data", 
       x = "Bill Depth (mm)", 
       y = "Bill Length (mm)" ) + 
  theme(legend.position = "none")

obs <- penguins %>% 
  ggplot(aes(y = bill_length_mm, x = bill_depth_mm, color = species)) + 
  geom_point() + 
  labs(title = "Observed Data", 
       x = "Bill Depth (mm)", 
       y = "Bill Length (mm)" ) + 
  theme(legend.position = "left")

gridExtra::grid.arrange(obs, pred, nrow = 1, widths = c(0.55, 0.45))
```

---

# Assessing Predictions 

We would expect that if the regression model is a good model for penguin bill 
lengths, then the simulated data should look similar to what was observed. We 
can compare the simulated data with the observed data using a scatterplot. 

If the simulated data were identical to the observed data, they would all fall 
on the $y = x$ line (in red). Values above the $y = x$ line correspond to
simulated bill lengths larger than the observed bill lengths, and visa versa. 
Overall, it appears that there are about as many over estimates as
underestimates, but the values are not terribly close to the line.

```{r}
new_data %>% 
  ggplot(aes(x = predict_length, y = bill_length_mm)) + 
  geom_point() + 
   labs(x = "Observed Bill Length (mm)", 
       y = "Simulated Bill Length (mm)" ) + 
  geom_abline(slope = 1, intercept = 0, 
              color = "red", linetype = "dashed", lwd = 1.5)

```

We can use a statistic to summarize how "close" the simulated values and the 
observed values are. Any statistic that captures the residuals from this 
regression would do, but I prefer $R^2,$ since it has an easy reference value 
for what I would expect if my model did a good job ($R^2 = 1$). 

I like the `glance()` function from the **broom** package, since it produces 
a nice table output of the summary measures from a linear regression. I can 
then use the column names of the table to select the `r.squared` column. 
s
```{r}
new_data %>% 
  lm(bill_length_mm ~ predict_length, data = .) %>% 
  glance() %>% 
  select(r.squared) %>% 
  pull()
```

`r emo::ji("check")` **Check-in: Interpreting $R^2$**

What does the $R^2$ value for the above regression mean? How would you 
interpret it? 

---


# Iterating! 

We are interested in seeing how our model performs for more than one simulated 
dataset. Thus, we need to iterate through the process outlined above.
Specifically, at each step we will need to:

1. Simulate new data
2. Regress the new data on the observed data
3. Save the $R^2$ from the regression 

### Lots of Simulated Observations

Luckily, we have already written the `noise()` function that helps us simulate
new observations that could have occurred under our model. Since we are not
changing any input to `noise()`, we can choose between passing an "arbitrary" 
input to `map()` (e.g., `1:100`). This essentially rerun the same process
multiple times. Since we want for every simulated dataset to be bound together 
as columns, we can use the `map__dfc()` function! 

```{r}
nsims <- 100

sims <- map_dfc(1:nsims,
                ~tibble(sim = noise(penguin_predict, sd = penguin_sigma))) 
```

Since all of the columns have the same name, **dplyr** automatically adds `...` 
and a number after each column. We can replace these `...`s with 
`str_replace_all()`! Remember, the `.` is a special character and needs to be 
escaped! 

```{r}
colnames(sims) <- colnames(sims) %>% 
  str_replace(pattern = "\\.\\.\\.",
                  replace = "_")
```

Finally, we can add the observed into our simulated dataset, for ease of 
modeling and visualizing. 

```{r}
sims <- penguins %>% 
  filter(!is.na(bill_depth_mm), 
         !is.na(bill_length_mm), 
         !is.na(species)) %>% 
  select(bill_length_mm) %>% 
  bind_cols(sims)
```

### Lots of Regressions & $R^2$ Values

Now, we want to regress each of these simulated `bill_length`s on the original,
observed `bill_length_mm`. Again, we will need to iterate through this process. 

Before, I fit a linear regression between `bill_length_mm` and the simulated 
data. Now, I have 100 different datasets I need to regress on `bill_length_mm`. 
I can use `map()` to define a function that will be applied to each column of
`sims`. This function regresses `bill_length_mm` on each column (`.x`) from the 
`sims` dataset. 

Next, I `map()` the `glance()` function onto each of these 100 regressions. 
Finally, I use `map_dbl()` to extract the `r.squared` from each of the model 
fit summaries. 

```{r, results = 'hide'}
obs_vs_sim <- function(df){
  lm(penguins$bill_length_mm ~ x)
}

sim_r_sq <- sims %>% 
  map( ~lm(bill_length_mm ~ .x, data = sims)) %>% 
  map(glance) %>% 
  map_dbl(~.$r.squared)
```

`r emo::ji("check")` **Check-in: Mapping Regression Summaries**

What is the data structure of the values output after the `map(glance)` step? 
i.e. What data structure do the following lines return:
```
sims %>% 
  map( ~lm(bill_length_mm ~ .x, data = sims)) %>% 
  map(glance)
```

(a) list
(b) vector
(c) matrix
(d) dataframe


### Inspecting the $R^2$ Values

If I look at the first 6 $R^2$ values, I see that the first value
corresponds to the the following regression: 
`lm(bill_length_mm ~ bill_length_mm)`

All of the values thereafter are the $R^2$ values from the simulated data. 

```{r}
head(sim_r_sq)
```

I am interested in looking at the distribution of $R^2$ values from the 
simulated data, so I'm going to remove this unwanted entry of the `sim_r_sq`
vector. 

```{r}
sim_r_sq <- sim_r_sq[names(sim_r_sq) != "bill_length_mm"]
```


## Plotting the Simulated $R^2$ Values versus the Observed $R^2$

The final stage is to plot the statistics from the simulated data. The
distribution of these $R^2$ values will tell if our assumed model does a good 
job of producing data similar to what was observed. If the model produces data 
similar to what was observed, we would expect $R^2$ values near 1. 

```{r}
tibble(sims = sim_r_sq) %>% 
  ggplot(aes(x = sims)) + 
  geom_histogram(binwidth = 0.0105)
```

In this plot, we see that the simulated datasets have $R^2$ vales between 0.6
and 0.7. To me, this indicates that the data simulated under this statistical 
model are moderately similar to what was observed. On average, our statistical 
model does not account for about 35% of the observed variability in the
observed penguin bill lengths. 



